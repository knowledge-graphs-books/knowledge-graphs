{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'SPARQLWrapper'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mSPARQLWrapper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SPARQLWrapper, CSV\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StringIO\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'SPARQLWrapper'"
     ]
    }
   ],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, CSV\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Define the SPARQL endpoint and query\n",
    "endpoint_url = \"https://dbpedia.org/sparql\"\n",
    "def make_query(batch_size = 1000, offset = 0):\n",
    "  return\"\"\"\n",
    "PREFIX dbo: <http://dbpedia.org/ontology/>\n",
    "PREFIX dbr: <http://dbpedia.org/resource/>\n",
    "PREFIX dbp: <http://dbpedia.org/property/>\n",
    "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
    "\n",
    "SELECT * \n",
    "WHERE {\n",
    "  {\n",
    "    SELECT \n",
    "      ?authorLabel\n",
    "      (GROUP_CONCAT(DISTINCT ?allGenres; separator=\", \") AS ?combinedGenres)\n",
    "      ?birthDate\n",
    "      (COALESCE(?nationalityLabel, ?languageResourceLabel, ?languageLiteral) AS ?combinedNationalityOrLanguage)\n",
    "      (GROUP_CONCAT(DISTINCT ?topBookLabel; separator=\", \") AS ?topBooks)\n",
    "    WHERE {\n",
    "      ?author a dbo:Writer ;\n",
    "              rdfs:label ?authorLabel ;\n",
    "              dbo:birthDate ?birthDate.\n",
    "      FILTER (lang(?authorLabel) = \"en\")\n",
    "\n",
    "      # Genres from dbo:genre\n",
    "      OPTIONAL {\n",
    "        ?author dbo:genre ?genre.\n",
    "        ?genre rdfs:label ?genreLabel2.\n",
    "        FILTER (lang(?genreLabel2) = \"en\")\n",
    "        BIND(?genreLabel2 AS ?allGenres)\n",
    "      }\n",
    "\n",
    "      # Genres from dbp:genre (literal)\n",
    "      OPTIONAL {\n",
    "        ?author dbp:genre ?plainGenre.\n",
    "        BIND(STR(?plainGenre) AS ?genreLabel)\n",
    "        BIND(?genreLabel AS ?allGenres)\n",
    "      }\n",
    "\n",
    "      # Nationality\n",
    "      OPTIONAL {\n",
    "        ?author dbo:nationality ?nationality.\n",
    "        ?nationality rdfs:label ?nationalityLabel.\n",
    "        FILTER (lang(?nationalityLabel) = \"en\")\n",
    "      }\n",
    "\n",
    "      # Language from dbo:language\n",
    "      OPTIONAL {\n",
    "        ?author dbo:language ?langResource.\n",
    "        ?langResource rdfs:label ?languageResourceLabel.\n",
    "        FILTER (lang(?languageResourceLabel) = \"en\")\n",
    "      }\n",
    "\n",
    "      # Language from dbp:language (literal)\n",
    "      OPTIONAL {\n",
    "        ?author dbp:language ?languageLit.\n",
    "        FILTER (lang(?languageLit) = \"en\")\n",
    "        BIND(STR(?languageLit) AS ?languageLiteral)\n",
    "      }\n",
    "\n",
    "      # Notable works\n",
    "      OPTIONAL {\n",
    "        { ?author dbo:notableWork ?topBook }\n",
    "        UNION\n",
    "        { ?author dbp:notableWork ?topBook }\n",
    "        ?topBook rdfs:label ?topBookLabel.\n",
    "        FILTER (lang(?topBookLabel) = \"en\")\n",
    "      }\n",
    "    }\n",
    "    GROUP BY ?authorLabel ?birthDate ?nationalityLabel ?languageResourceLabel ?languageLiteral\n",
    "    ORDER BY ?authorLabel\n",
    "    \"\"\" +  f\"LIMIT {batch_size} OFFSET {offset}\" + \"\"\"\n",
    "  }\n",
    "  FILTER (\n",
    "    ?combinedGenres != \"\" && \n",
    "    ?combinedNationalityOrLanguage != \"\" && \n",
    "    ?topBooks != \"\"\n",
    "  )\n",
    "}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c0cff38e3634c988ccfa2b5cf149286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching pages:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows acquired in this batch: 25 | Total rows: 25\n",
      "Rows acquired in this batch: 27 | Total rows: 52\n",
      "Rows acquired in this batch: 36 | Total rows: 88\n",
      "Rows acquired in this batch: 32 | Total rows: 120\n",
      "Rows acquired in this batch: 30 | Total rows: 150\n",
      "Rows acquired in this batch: 31 | Total rows: 181\n",
      "Rows acquired in this batch: 35 | Total rows: 216\n",
      "Rows acquired in this batch: 30 | Total rows: 246\n",
      "Rows acquired in this batch: 37 | Total rows: 283\n",
      "Rows acquired in this batch: 43 | Total rows: 326\n",
      "Rows acquired in this batch: 39 | Total rows: 365\n",
      "Rows acquired in this batch: 41 | Total rows: 406\n",
      "Rows acquired in this batch: 31 | Total rows: 437\n",
      "Rows acquired in this batch: 32 | Total rows: 469\n",
      "Rows acquired in this batch: 43 | Total rows: 512\n",
      "Rows acquired in this batch: 26 | Total rows: 538\n",
      "Rows acquired in this batch: 37 | Total rows: 575\n",
      "Rows acquired in this batch: 27 | Total rows: 602\n",
      "Rows acquired in this batch: 31 | Total rows: 633\n",
      "Rows acquired in this batch: 34 | Total rows: 667\n",
      "Rows acquired in this batch: 23 | Total rows: 690\n",
      "Rows acquired in this batch: 29 | Total rows: 719\n",
      "Rows acquired in this batch: 46 | Total rows: 765\n",
      "Rows acquired in this batch: 47 | Total rows: 812\n",
      "Rows acquired in this batch: 39 | Total rows: 851\n",
      "Rows acquired in this batch: 36 | Total rows: 887\n",
      "Rows acquired in this batch: 23 | Total rows: 910\n",
      "Rows acquired in this batch: 38 | Total rows: 948\n",
      "Rows acquired in this batch: 28 | Total rows: 976\n",
      "Rows acquired in this batch: 33 | Total rows: 1009\n",
      "Rows acquired in this batch: 28 | Total rows: 1037\n",
      "Rows acquired in this batch: 31 | Total rows: 1068\n",
      "Rows acquired in this batch: 28 | Total rows: 1096\n",
      "Rows acquired in this batch: 21 | Total rows: 1117\n",
      "Rows acquired in this batch: 32 | Total rows: 1149\n",
      "Rows acquired in this batch: 22 | Total rows: 1171\n",
      "Rows acquired in this batch: 28 | Total rows: 1199\n",
      "Rows acquired in this batch: 22 | Total rows: 1221\n",
      "Rows acquired in this batch: 27 | Total rows: 1248\n",
      "Rows acquired in this batch: 27 | Total rows: 1275\n",
      "Rows acquired in this batch: 28 | Total rows: 1303\n",
      "Rows acquired in this batch: 29 | Total rows: 1332\n",
      "Rows acquired in this batch: 51 | Total rows: 1383\n",
      "Rows acquired in this batch: 32 | Total rows: 1415\n",
      "Rows acquired in this batch: 21 | Total rows: 1436\n",
      "Rows acquired in this batch: 32 | Total rows: 1468\n",
      "Rows acquired in this batch: 33 | Total rows: 1501\n",
      "Rows acquired in this batch: 28 | Total rows: 1529\n",
      "Rows acquired in this batch: 42 | Total rows: 1571\n",
      "Rows acquired in this batch: 29 | Total rows: 1600\n",
      "Rows acquired in this batch: 38 | Total rows: 1638\n",
      "Rows acquired in this batch: 21 | Total rows: 1659\n",
      "Rows acquired in this batch: 10 | Total rows: 1669\n",
      "Rows acquired in this batch: 0 | Total rows: 1669\n",
      "Rows acquired in this batch: 0 | Total rows: 1669\n",
      "Rows acquired in this batch: 0 | Total rows: 1669\n",
      "Rows acquired in this batch: 0 | Total rows: 1669\n",
      "Rows acquired in this batch: 0 | Total rows: 1669\n",
      "Rows acquired in this batch: 0 | Total rows: 1669\n",
      "Rows acquired in this batch: 0 | Total rows: 1669\n",
      "Reached maximum pages defined.\n",
      "Total rows acquired: 1669\n",
      "              authorLabel          combinedGenres   birthDate  \\\n",
      "0             A. A. Milne   Children's literature  1882-01-18   \n",
      "1           A. E. Housman            Lyric poetry  1859-03-26   \n",
      "2        A. Igoni Barrett   Short stories, novels  1979-03-26   \n",
      "3             A. J. Healy         Comedy, Fantasy  1969-04-02   \n",
      "4          A. J. Quinnell        Thriller fiction  1940-06-25   \n",
      "...                   ...                     ...         ...   \n",
      "1664   Álvares de Azevedo  Essay, Poetry, Theatre  1831-09-12   \n",
      "1665         Álvaro Mutis                 Fiction  1923-08-25   \n",
      "1666         Åke Holmberg   Children's literature  1907-05-31   \n",
      "1667  Élisabeth Vonarburg     Fantasy, Historical  1947-08-05   \n",
      "1668       Émile Gaboriau       Detective fiction  1832-11-09   \n",
      "\n",
      "     combinedNationalityOrLanguage  \\\n",
      "0                              NaN   \n",
      "1                              NaN   \n",
      "2                          English   \n",
      "3                          Ireland   \n",
      "4                              NaN   \n",
      "...                            ...   \n",
      "1664           Portuguese language   \n",
      "1665                           NaN   \n",
      "1666              Swedish language   \n",
      "1667                           NaN   \n",
      "1668                           NaN   \n",
      "\n",
      "                                               topBooks  \n",
      "0                                       Winnie-the-Pooh  \n",
      "1                                      A Shropshire Lad  \n",
      "2                                              Blackass  \n",
      "3     Tommy Storm, Tommy Storm and the Galactic Knights  \n",
      "4               Man on Fire (Quinnell novel), The Mahdi  \n",
      "...                                                 ...  \n",
      "1664              Lira dos Vinte Anos, Noite na Taverna  \n",
      "1665        The Adventures and Misadventures of Maqroll  \n",
      "1666                                        Tam Sventon  \n",
      "1667                       Chroniques du Pays des Mères  \n",
      "1668                             Monsieur Lecoq (novel)  \n",
      "\n",
      "[1669 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from SPARQLWrapper import SPARQLWrapper, CSV\n",
    "from io import StringIO\n",
    "\n",
    "# SPARQL Endpoint and Query\n",
    "endpoint_url = \"https://dbpedia.org/sparql\"\n",
    "batch_size = 500\n",
    "offset = 0\n",
    "total_rows = 0  # Counter for the total rows\n",
    "\n",
    "sparql = SPARQLWrapper(endpoint_url)\n",
    "sparql.setReturnFormat(CSV)\n",
    "sparql.setTimeout(300)\n",
    "# Initialize an empty DataFrame to store all results\n",
    "all_data = pd.DataFrame()\n",
    "\n",
    "# Estimated total pages\n",
    "max_pages = 60\n",
    "\n",
    "# Progress bar setup\n",
    "with tqdm(total=max_pages, desc=\"Fetching pages\") as pbar:\n",
    "    while True:\n",
    "        # Update the query with the current OFFSET\n",
    "        paginated_query = make_query(batch_size, offset)\n",
    "        sparql.setQuery(paginated_query)\n",
    "\n",
    "        # Execute the query and fetch results\n",
    "        results = sparql.query().convert()\n",
    "        csv_str = results.decode(\"utf-8\")\n",
    "        data = pd.read_csv(StringIO(csv_str))\n",
    "\n",
    "        # If no data is returned, stop the loop\n",
    "        # if data.empty:\n",
    "        #     print(f\"No more data after offset {offset}.\")\n",
    "        #     break\n",
    "\n",
    "        # Append the current batch to the all_data DataFrame\n",
    "        all_data = pd.concat([all_data, data], ignore_index=True)\n",
    "\n",
    "        # Update total rows count\n",
    "        total_rows += len(data)\n",
    "        print(f\"Rows acquired in this batch: {len(data)} | Total rows: {total_rows}\")\n",
    "\n",
    "        # Increment the offset for the next batch\n",
    "        offset += batch_size\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.update(1)\n",
    "\n",
    "        # Stop after a maximum number of pages (safety to prevent infinite loops)\n",
    "        if pbar.n >= max_pages:\n",
    "            print(\"Reached maximum pages defined.\")\n",
    "            break\n",
    "\n",
    "\n",
    "        # Optionally, save to CSV\n",
    "        all_data.to_csv(\"tmp_authors_data.csv\", index=False)\n",
    "        \n",
    "\n",
    "# Print the final DataFrame and total rows\n",
    "print(f\"Total rows acquired: {total_rows}\")\n",
    "print(all_data)\n",
    "\n",
    "all_data.to_csv(\"authors_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from rdflib import Graph, Namespace, Literal, RDF, URIRef\n",
    "from rdflib.namespace import XSD, RDFS\n",
    "import datetime\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# --------------------------\n",
    "# Helper functions\n",
    "# --------------------------\n",
    "\n",
    "def split_and_clean(s):\n",
    "    \"\"\"\n",
    "    Utility function to split comma or newline separated strings and clean them up.\n",
    "    \"\"\"\n",
    "    if pd.isna(s):\n",
    "        return []\n",
    "    items = []\n",
    "    # Split by commas, then split by newlines if necessary\n",
    "    for item in s.split(\",\"):\n",
    "        items.extend(item.split(\"\\n\"))\n",
    "    # Remove punctuation, strip spaces, and title-case\n",
    "    clean_items = [re.sub(r'[^a-zA-Z0-9 ]', '', x.strip()).strip().title() for x in items if x.strip()]\n",
    "    return clean_items\n",
    "\n",
    "def safe_uri(base, raw_name):\n",
    "    \"\"\"\n",
    "    Convert a raw string to a safe URI by removing or encoding special characters and lowercasing.\n",
    "    \"\"\"\n",
    "    # Replace spaces with underscores, remove non-alphanumeric underscores, then lowercase\n",
    "    clean_name = re.sub(r'[^a-zA-Z0-9_]+', '', raw_name.replace(\" \", \"_\")).lower()\n",
    "    return URIRef(base + clean_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Invalid date format '-065-12-08' for author 'Horace'. Skipping.\n",
      "Ontology successfully saved as authors_ontology_20241217_184531.owl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read CSV\n",
    "df = pd.read_csv(\"authors_fixed.csv\")\n",
    "\n",
    "# Pre-process data\n",
    "df[\"author_id\"] = df['authorLabel'].astype(str)  # Create a unique ID for each author\n",
    "df['genres_list'] = df['combinedGenres'].apply(split_and_clean)\n",
    "df['books_list']  = df['topBooks'].apply(split_and_clean)\n",
    "df = df.drop_duplicates(subset=['author_id'])\n",
    "\n",
    "# Initialize an RDFLib graph\n",
    "graph = Graph()\n",
    "\n",
    "# Define the base namespace for our ontology\n",
    "BASE = \"http://www.semanticweb.org/admin/ontologies/2024/9/authors-ontology#\"\n",
    "ONS = Namespace(BASE)\n",
    "graph.bind(\"ons\", ONS)\n",
    "\n",
    "# Define our classes\n",
    "AUTHOR_CLASS = ONS.Author\n",
    "GENRE_CLASS  = ONS.Genre\n",
    "BOOK_CLASS   = ONS.Book\n",
    "NATLANG_CLASS = ONS.NationalityOrLanguage\n",
    "\n",
    "# Add RDF:Class statements to the graph for clarity\n",
    "graph.add((AUTHOR_CLASS, RDF.type, RDFS.Class))\n",
    "graph.add((GENRE_CLASS,  RDF.type, RDFS.Class))\n",
    "graph.add((BOOK_CLASS,   RDF.type, RDFS.Class))\n",
    "graph.add((NATLANG_CLASS, RDF.type, RDFS.Class))\n",
    "\n",
    "# Define properties (object/data properties)\n",
    "# For example: hasGenre, wroteBook, hasNationality, name, birthDate, etc.\n",
    "HAS_GENRE       = ONS.hasGenre\n",
    "WROTE_BOOK      = ONS.wroteBook\n",
    "HAS_NATLANG     = ONS.hasNationalityOrLanguage\n",
    "NAME            = ONS.name\n",
    "BIRTH_DATE      = ONS.birthDate\n",
    "\n",
    "# A helper dictionary to store references to existing URIs (for re-use)\n",
    "# This helps avoid duplicating the same genre/book/nationality resource\n",
    "known_genres = {}\n",
    "known_books  = {}\n",
    "known_natlang = {}\n",
    "\n",
    "def add_genre(genre_name):\n",
    "    \"\"\"\n",
    "    Create or retrieve a genre resource in the graph.\n",
    "    \"\"\"\n",
    "    if genre_name not in known_genres:\n",
    "        genre_uri = safe_uri(BASE, f\"genre_{genre_name}\")\n",
    "        # Add triple: (genre_uri, RDF.type, GENRE_CLASS)\n",
    "        graph.add((genre_uri, RDF.type, GENRE_CLASS))\n",
    "        # Add triple: (genre_uri, RDFS.label, Literal(genre_name))\n",
    "        graph.add((genre_uri, RDFS.label, Literal(genre_name)))\n",
    "        # You could also store a 'name' property\n",
    "        graph.add((genre_uri, NAME, Literal(genre_name)))\n",
    "        known_genres[genre_name] = genre_uri\n",
    "    return known_genres[genre_name]\n",
    "\n",
    "def add_book(book_title):\n",
    "    \"\"\"\n",
    "    Create or retrieve a book resource in the graph.\n",
    "    \"\"\"\n",
    "    if book_title not in known_books:\n",
    "        book_uri = safe_uri(BASE, f\"book_{book_title}\")\n",
    "        graph.add((book_uri, RDF.type, BOOK_CLASS))\n",
    "        graph.add((book_uri, RDFS.label, Literal(book_title)))\n",
    "        graph.add((book_uri, NAME, Literal(book_title)))\n",
    "        known_books[book_title] = book_uri\n",
    "    return known_books[book_title]\n",
    "\n",
    "def add_natlang(natlang):\n",
    "    \"\"\"\n",
    "    Create or retrieve a NationalityOrLanguage resource in the graph.\n",
    "    \"\"\"\n",
    "    if natlang not in known_natlang:\n",
    "        natlang_uri = safe_uri(BASE, f\"natlang_{natlang}\")\n",
    "        graph.add((natlang_uri, RDF.type, NATLANG_CLASS))\n",
    "        graph.add((natlang_uri, RDFS.label, Literal(natlang)))\n",
    "        graph.add((natlang_uri, NAME, Literal(natlang)))\n",
    "        known_natlang[natlang] = natlang_uri\n",
    "    return known_natlang[natlang]\n",
    "\n",
    "def add_author(row):\n",
    "    \"\"\"\n",
    "    Create or retrieve an Author resource, and add properties and links.\n",
    "    \"\"\"\n",
    "    author_name = row[\"authorLabel\"]\n",
    "    author_uri = safe_uri(BASE, f\"author_{author_name}\")\n",
    "    \n",
    "    # Add type triple\n",
    "    graph.add((author_uri, RDF.type, AUTHOR_CLASS))\n",
    "    # Add name triple\n",
    "    graph.add((author_uri, NAME, Literal(author_name)))\n",
    "\n",
    "    # If birthDate is present and parseable, add as date literal\n",
    "    birth_str = row.get(\"birthDate\", \"\")\n",
    "    if pd.notna(birth_str) and birth_str.strip():\n",
    "        try:\n",
    "            # Validate the date string\n",
    "            parsed_date = datetime.strptime(birth_str, \"%Y-%m-%d\")\n",
    "            # If valid, add it as an XSD.date literal\n",
    "            graph.add((author_uri, BIRTH_DATE, Literal(parsed_date.date(), datatype=XSD.date)))\n",
    "        except ValueError:\n",
    "            # Skip invalid dates and print a warning\n",
    "            print(f\"Warning: Invalid date format '{birth_str}' for author '{row['authorLabel']}'. Skipping.\")\n",
    "\n",
    "    \n",
    "    # If there's nationality/language, add it\n",
    "    natlang = row.get(\"combinedNationalityOrLanguage\", \"\")\n",
    "    if pd.notna(natlang) and natlang.strip():\n",
    "        # We can further split if you suspect multiple nationalities in one row\n",
    "        # but the sample only seems to have one. We'll treat it as a single string.\n",
    "        # If multiple, you'd parse similarly to how we parse genres/books.\n",
    "        natlang_uri = add_natlang(natlang.title().strip())\n",
    "        graph.add((author_uri, HAS_NATLANG, natlang_uri))\n",
    "\n",
    "    # Add genre links\n",
    "    for g in row['genres_list']:\n",
    "        genre_uri = add_genre(g)\n",
    "        graph.add((author_uri, HAS_GENRE, genre_uri))\n",
    "    \n",
    "    # Add book links\n",
    "    for b in row['books_list']:\n",
    "        book_uri = add_book(b)\n",
    "        graph.add((author_uri, WROTE_BOOK, book_uri))\n",
    "\n",
    "# Iterate over each author row and add them to the graph\n",
    "for idx, row in df.iterrows():\n",
    "    add_author(row)\n",
    "\n",
    "# Finally, serialize to an .owl (RDF/XML) file\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "OUTPUT_OWL_FILE = f\"authors_ontology_{timestamp}.owl\"\n",
    "\n",
    "try:\n",
    "    graph.serialize(destination=OUTPUT_OWL_FILE, format='application/rdf+xml')\n",
    "    print(f\"Ontology successfully saved as {OUTPUT_OWL_FILE}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to save ontology: {e}\")\n",
    "    sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of triples: 24692\n",
      "Number of distinct classes: 4\n",
      "Number of distinct properties: 7\n",
      "Number of authors: 1629\n",
      "Number of genres: 674\n",
      "Number of books: 3408\n",
      "Number of nationalities or languages: 165\n"
     ]
    }
   ],
   "source": [
    "num_triples = len(graph)\n",
    "distinct_classes = set(graph.subjects(RDF.type, RDFS.Class))\n",
    "distinct_properties = set(graph.predicates())\n",
    "\n",
    "# You can also count specific instances like authors, books, etc.\n",
    "num_authors = len(set(graph.subjects(RDF.type, AUTHOR_CLASS)))\n",
    "num_genres = len(set(graph.subjects(RDF.type, GENRE_CLASS)))\n",
    "num_books = len(set(graph.subjects(RDF.type, BOOK_CLASS)))\n",
    "num_natlangs = len(set(graph.subjects(RDF.type, NATLANG_CLASS)))\n",
    "\n",
    "print(\"Total number of triples:\", num_triples)\n",
    "print(\"Number of distinct classes:\", len(distinct_classes))\n",
    "print(\"Number of distinct properties:\", len(distinct_properties))\n",
    "print(\"Number of authors:\", num_authors)\n",
    "print(\"Number of genres:\", num_genres)\n",
    "print(\"Number of books:\", num_books)\n",
    "print(\"Number of nationalities or languages:\", num_natlangs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
